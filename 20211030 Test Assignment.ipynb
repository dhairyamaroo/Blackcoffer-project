{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5681841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "from textstat.textstat import textstatistics\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfccba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the stopwords to a single csv file\n",
    "\n",
    "headers=['stopwords']\n",
    "StopWords_Auditor=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/StopWords_Auditor.txt\", sep='|', encoding='latin-1',names=headers)\n",
    "\n",
    "StopWords_Currencies=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/StopWords_Currencies.txt\", sep='|', encoding='latin-1')\n",
    "StopWords_Currencies.columns =['stopwords','xyz']\n",
    "StopWords_Currencies.drop(StopWords_Currencies.columns[[1]],axis=1 ,inplace=True)\n",
    "\n",
    "StopWords_DatesandNumbers=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/StopWords_DatesandNumbers.txt\", sep='|', encoding='latin-1')\n",
    "StopWords_DatesandNumbers.columns =['stopwords','xyz']\n",
    "StopWords_DatesandNumbers.drop(StopWords_DatesandNumbers.columns[[1]],axis=1 ,inplace=True)\n",
    "\n",
    "StopWords_Geographic=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/StopWords_Geographic.txt\", sep='|',encoding='latin-1')\n",
    "StopWords_Geographic.columns =['stopwords','xyz']\n",
    "StopWords_Geographic.drop(StopWords_Geographic.columns[[1]],axis=1 ,inplace=True)\n",
    "\n",
    "StopWords_Generic=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/StopWords_Generic.txt\", sep='|',encoding='latin-1',names=headers)\n",
    "\n",
    "StopWords_GenericLong=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/StopWords_GenericLong.txt\", sep='|',encoding='latin-1',names=headers)\n",
    "\n",
    "StopWords_Names=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/StopWords_Names.txt\", sep='|',encoding='latin-1')\n",
    "StopWords_Names.columns =['stopwords','xyz']\n",
    "StopWords_Names.drop(StopWords_Names.columns[[1]],axis=1 ,inplace=True)\n",
    "\n",
    "concate_data = pd.concat([StopWords_Auditor,StopWords_Currencies,StopWords_DatesandNumbers,StopWords_Generic,StopWords_GenericLong,StopWords_Geographic,StopWords_Names])\n",
    "concate_data.to_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/stopwords.txt\")\n",
    "stoplist=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/StopWords/stopwords.txt\")\n",
    "stoplist.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "stoplist['stopwords'] = stoplist['stopwords'].str.lower()\n",
    "stoplist=(stoplist['stopwords'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03900488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract next from url\n",
    "\n",
    "df=pd.read_excel(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/Output Data Structure.xlsx\")\n",
    "\n",
    "\n",
    "def break_sentences(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    return list(doc.sents)\n",
    "    \n",
    "        # Returns Number of Words in the text\n",
    "def word_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words\n",
    "\n",
    "        # Returns the number of sentences in the text\n",
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    return len(sentences)\n",
    "\n",
    "        # Returns average sentence length\n",
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    "\n",
    "\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "\n",
    "        # Returns the average number of syllables per word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "    return legacy_round(ASPW, 1)\n",
    "\n",
    "        # Return total Difficult Words in a text\n",
    "def difficult_words(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "            # Find all words in the text\n",
    "    words = []\n",
    "    sentences = break_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        words += [str(token) for token in sentence]\n",
    "\n",
    "            # difficult words are those with syllables >= 2\n",
    "            # easy_word_set is provide by Textstat as\n",
    "            # a list of common words\n",
    "    diff_words_set = set()\n",
    "\n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    "\n",
    "    return len(diff_words_set)\n",
    "\n",
    "\n",
    "def avgWordLen(token_text):    \n",
    "    letter_count=0\n",
    "    for i in token_text:\n",
    "        for j in i:\n",
    "            letter_count+=1\n",
    "    avg_word_len=letter_count/len(token_text)\n",
    "    return avg_word_len\n",
    "\n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    "    return len(diff_words_set)\n",
    "\n",
    "def personalPronouns(token_text):\n",
    "    count=0\n",
    "    for w in token_text:\n",
    "        if w in ['I','we','my','ours','us']:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def avgWordLen(new_token_text):    \n",
    "    letter_count=0\n",
    "    for i in new_token_text:\n",
    "        for j in i:\n",
    "            letter_count+=1\n",
    "    avg_word_len=letter_count/len(new_token_text)\n",
    "    return avg_word_len\n",
    "\n",
    "\n",
    "\n",
    "input_data = pd.read_excel(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/Output Data Structure.xlsx\")\n",
    "for i in range(len(input_data)):\n",
    "    print(i)\n",
    "    htmldata = requests.get(input_data.URL[i]).text\n",
    "    soup = BeautifulSoup(htmldata, 'html.parser')\n",
    "    text=''\n",
    "    data=''\n",
    "    for data in soup.find_all(\"p\"):\n",
    "        text+=data.get_text()\n",
    "        text+=''\n",
    "        \n",
    "    if len(text)>0:\n",
    "        #tokenize   \n",
    "        token_text=word_tokenize(text)\n",
    "\n",
    "        #clean\n",
    "        clean_list=[]\n",
    "        for word in token_text:\n",
    "            if word not in stoplist:\n",
    "                clean_list.append(word)\n",
    "        # 1.3 Extracting Derived variables\n",
    "\n",
    "        #positiveScore\n",
    "        header=['positive']\n",
    "        positive=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/MasterDictionary/positive-words.txt\",encoding='latin-1',names=header)\n",
    "        positive=positive['positive'].tolist()\n",
    "        positiveScore=0\n",
    "        for word in clean_list:\n",
    "            if word in positive:\n",
    "                positiveScore+=1\n",
    "        df.loc[i,'POSITIVE SCORE'] = positiveScore\n",
    "        print(positiveScore)\n",
    "\n",
    "\n",
    "        #negetiveScore\n",
    "        header=['negative']\n",
    "        negative=pd.read_csv(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/MasterDictionary/negative-words.txt\",encoding='latin-1',names=header)\n",
    "        negative=negative['negative'].tolist()\n",
    "        negativeScore=0\n",
    "        for word in clean_list:\n",
    "            if word in negative:\n",
    "                 negativeScore+=1\n",
    "        df.loc[i,'NEGATIVE SCORE'] = negativeScore\n",
    "        print(negativeScore)\n",
    "\n",
    "\n",
    "        #polarity_score\n",
    "        polarity_score=(positiveScore - negativeScore)/ ((positiveScore+ negativeScore) + 0.000001)\n",
    "        df.loc[i,'POLARITY SCORE'] = polarity_score\n",
    "        print(polarity_score)\n",
    "\n",
    "\n",
    "        # Subjectivity score\n",
    "        Subjectivity_Score = (positiveScore + negativeScore)/ ((len(clean_list)) + 0.000001)\n",
    "        df.loc[i,'SUBJECTIVITY SCORE'] = Subjectivity_Score\n",
    "        print(Subjectivity_Score)\n",
    "\n",
    "\n",
    "        per_diff_words = (difficult_words(text) / word_count(text) * 100) + 5\n",
    "        grade = 0.4 * (avg_sentence_length(text) + per_diff_words)\n",
    "\n",
    "\n",
    "        print(avg_sentence_length(text))\n",
    "        df.loc[i,'AVG SENTENCE LENGTH'] = avg_sentence_length(text)\n",
    "\n",
    "        print(per_diff_words)\n",
    "        df.loc[i,'PERCENTAGE OF COMPLEX WORDS'] = per_diff_words\n",
    "\n",
    "        print(grade)\n",
    "        df.loc[i,'FOG INDEX'] = grade\n",
    "\n",
    "        Avg_Word_persentence=len(token_text)/sentence_count(text)\n",
    "        df.loc[i,'AVG NUMBER OF WORDS PER SENTENCE'] = Avg_Word_persentence\n",
    "        print(Avg_Word_persentence)\n",
    "\n",
    "        df.loc[i,'COMPLEX WORD COUNT'] = difficult_words(text)\n",
    "        difficult_words(text)\n",
    "\n",
    "        from nltk.corpus import stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_sentence = [w for w in token_text if not w.lower() in stop_words]\n",
    "        #with no lower case conversion\n",
    "        filtered_sentence = []\n",
    "\n",
    "        for w in token_text:\n",
    "            if w not in (stop_words and [\"!\",\"“\",\",\",\"”\",\"’\",\"'\",\"[\",\"]\",\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"-\",\"_\",\"{\",\"}\",\":\",\";\",\"?\",\".\"]):\n",
    "                filtered_sentence.append(w)\n",
    "\n",
    "\n",
    "        df.loc[i,'WORD COUNT'] = len(filtered_sentence)\n",
    "        print(len(filtered_sentence))\n",
    "\n",
    "        df.loc[i,'SYLLABLE PER WORD'] = syllables_count(text)\n",
    "        syllables_count(text)\n",
    "\n",
    "        df.loc[i,'PERSONAL PRONOUNS'] = personalPronouns(text)\n",
    "        personalPronouns(text)\n",
    "\n",
    "        df.loc[i,'AVG WORD LENGTH'] = avgWordLen(token_text)\n",
    "        avgWordLen(token_text)\n",
    "        result = [i for i in df if i.startswith('Unnamed')]\n",
    "        df.to_excel(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/Output Data Structure.xlsx\")\n",
    "        for i in result:\n",
    "            df.drop([i], axis=1, inplace=True)\n",
    "        df.to_excel(\"/Users/dhairyamaroo/Downloads/20211030 Test Assignment/Output Data Structure.xlsx\")\n",
    "\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e72013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>27.418919</td>\n",
       "      <td>31.811237</td>\n",
       "      <td>23.692062</td>\n",
       "      <td>26.297297</td>\n",
       "      <td>544.0</td>\n",
       "      <td>1758.0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>5.281603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.191011</td>\n",
       "      <td>0.107748</td>\n",
       "      <td>21.324675</td>\n",
       "      <td>21.930572</td>\n",
       "      <td>17.302099</td>\n",
       "      <td>20.896104</td>\n",
       "      <td>278.0</td>\n",
       "      <td>1409.0</td>\n",
       "      <td>2119.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.348664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.093511</td>\n",
       "      <td>22.619048</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>20.247619</td>\n",
       "      <td>22.130952</td>\n",
       "      <td>437.0</td>\n",
       "      <td>1673.0</td>\n",
       "      <td>2921.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.006993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>19.622222</td>\n",
       "      <td>23.006795</td>\n",
       "      <td>17.051607</td>\n",
       "      <td>19.177778</td>\n",
       "      <td>318.0</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>4.520278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.323077</td>\n",
       "      <td>0.063976</td>\n",
       "      <td>24.098765</td>\n",
       "      <td>26.413934</td>\n",
       "      <td>20.205080</td>\n",
       "      <td>23.506173</td>\n",
       "      <td>418.0</td>\n",
       "      <td>1699.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.633403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.106383</td>\n",
       "      <td>0.088847</td>\n",
       "      <td>22.234043</td>\n",
       "      <td>29.306220</td>\n",
       "      <td>20.616105</td>\n",
       "      <td>20.404255</td>\n",
       "      <td>254.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>1455.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.080292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>25.591837</td>\n",
       "      <td>25.972887</td>\n",
       "      <td>20.625889</td>\n",
       "      <td>24.551020</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1089.0</td>\n",
       "      <td>1757.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.816293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-0.235294</td>\n",
       "      <td>0.093923</td>\n",
       "      <td>20.253968</td>\n",
       "      <td>24.749216</td>\n",
       "      <td>18.001274</td>\n",
       "      <td>19.793651</td>\n",
       "      <td>252.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>1863.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.664796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.087613</td>\n",
       "      <td>25.760000</td>\n",
       "      <td>31.552795</td>\n",
       "      <td>22.925118</td>\n",
       "      <td>23.920000</td>\n",
       "      <td>171.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.301003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>26.002593</td>\n",
       "      <td>17.521037</td>\n",
       "      <td>17.307692</td>\n",
       "      <td>243.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.681778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              60.0            30.0        0.333333            0.075000   \n",
       "1              53.0            36.0        0.191011            0.107748   \n",
       "2              64.0            34.0        0.306122            0.093511   \n",
       "3              51.0            21.0        0.416667            0.084112   \n",
       "4              43.0            22.0        0.323077            0.063976   \n",
       "..              ...             ...             ...                 ...   \n",
       "109            21.0            26.0       -0.106383            0.088847   \n",
       "110            31.0             9.0        0.550000            0.059880   \n",
       "111            26.0            42.0       -0.235294            0.093923   \n",
       "112            26.0             3.0        0.793103            0.087613   \n",
       "113            30.0            36.0       -0.090909            0.107143   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              27.418919                    31.811237  23.692062   \n",
       "1              21.324675                    21.930572  17.302099   \n",
       "2              22.619048                    28.000000  20.247619   \n",
       "3              19.622222                    23.006795  17.051607   \n",
       "4              24.098765                    26.413934  20.205080   \n",
       "..                   ...                          ...        ...   \n",
       "109            22.234043                    29.306220  20.616105   \n",
       "110            25.591837                    25.972887  20.625889   \n",
       "111            20.253968                    24.749216  18.001274   \n",
       "112            25.760000                    31.552795  22.925118   \n",
       "113            17.800000                    26.002593  17.521037   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           26.297297               544.0      1758.0   \n",
       "1                           20.896104               278.0      1409.0   \n",
       "2                           22.130952               437.0      1673.0   \n",
       "3                           19.177778               318.0      1572.0   \n",
       "4                           23.506173               418.0      1699.0   \n",
       "..                                ...                 ...         ...   \n",
       "109                         20.404255               254.0       879.0   \n",
       "110                         24.551020               263.0      1089.0   \n",
       "111                         19.793651               252.0      1126.0   \n",
       "112                         23.920000               171.0       551.0   \n",
       "113                         17.307692               243.0      1026.0   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0               3140.0               38.0         5.281603  \n",
       "1               2119.0               35.0         4.348664  \n",
       "2               2921.0               29.0         5.006993  \n",
       "3               2421.0               61.0         4.520278  \n",
       "4               2730.0               32.0         4.633403  \n",
       "..                 ...                ...              ...  \n",
       "109             1455.0               13.0         5.080292  \n",
       "110             1757.0               13.0         4.816293  \n",
       "111             1863.0               16.0         4.664796  \n",
       "112              984.0                2.0         5.301003  \n",
       "113             1664.0                7.0         4.681778  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e88061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
